{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Support Vector Machines we divide a set of data points with a line or \"hyperplane\", which then is used to classify the data points\n",
    "\n",
    "The equation of the line is theoretically setup where if you plug in the data points into the equation, anything above 0 is positive and goes into one category, while anything negative goes into the other category.\n",
    "\n",
    "There can arise an issue however where there is theoretically an infinite number of lines that might even divide the data points into two categories. How this is solved is with Maximal Margin Classifiers; the distance between the line and it's closest point per category. This problem is solved by choosing the line with the furthest distances from the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the \"vectors\" are the points that actually impact the Maximal Margin Classifiers. Points far on the outside will never be the closest to the line so they are no vectors, but the points closer to the center are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrained Optimization: there is a function we want to optimize under a constraint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the hyperplane is established, imagine there are parallel lines drawn from the two nearest points from each category. The space between the hyperplane and those parallel lines is the margin.\n",
    "\n",
    "A support vector classifier loosens the contraints of the hyperplane and allows points to be on opposite sides of the hyperplane (but within the margin) from different categories. \n",
    "\n",
    "These are represented as \"slack variables\", which can sort of be thought of as error confidence. Points outside of the margin that are definitely correctly classified have a slack variable of 0. Values under the hyperplane within the margin have a slack below 1, and values above the hyperplane have a slack above 1. \n",
    "\n",
    "In the equation for determining the margins, there is a value \"c\" that represents the penalty given the slack variable. A smaller C makes the margin really large but is prone to underfitting. A larger value of C makes the margins smaller and can be prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, ConfusionMatrixDisplay, roc_auc_score, recall_score, precision_score\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1mSJFRHbiydTf3EvIIJr_kgw6Tfz1EXnb\" alt=\"hyperplane plot\" width = \"400\" class=\"center\"/> </center>\n",
    "\n",
    "In the lecture, we learned about Maximal Margin Classifiers, and Support Vector Classifiers. Both use hyperplanes (aka \"flat affine subspaces\") that divide our data into two sections.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1zAWlFxIOJchpnJRF3DGiIpv3VdFfq-ci\" alt=\"hyperplane plot\" width = \"800\" class=\"center\"/> </center>\n",
    "\n",
    "But if the data are linearly separable, there are infinite hyperplanes...HOW DO WE CHOOSE?\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1m5GZObdjAFCjMwldvfyfSJkZPABYZr9c\" alt=\"hyperplane plot\" width = \"400\" class=\"center\"/> </center>\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\" class=\"center\"/>\n",
    "\n",
    "Why is it beneficial to maximize the margin as a way to separate two groups with a hyper plane?\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "But Maximal Margin Classifiers have a problem...\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\" class=\"center\"/>\n",
    "\n",
    "How to SVCs solve the major issue with MMCs?\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=13OZpzK4-BCqjVsquR7XClrvHf64cGGaC\" alt=\"Q\" width = \"400\" class=\"center\"/> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Constrained Optimization (Lagrangians)\n",
    "\n",
    "## Maximal Margin Classifier $\\mathcal{L}$\n",
    "For Maximal Margin Classifiers, we're trying to *maximize* the *minimum* distance between the closest point(s) and the dividing hyperpalne. In otherwords, we're maximizing the function:\n",
    "\n",
    "$$\\underbrace{\\frac{1}{||\\mathbf{w}||} \\underset{n}{\\min}\\left [ t_n(\\mathbf{w}^Tx_n + b) \\right ]}_\\text{minimum distance from a point to hyperplane}$$\n",
    "\n",
    "which is equivalent to minimizing $^1$:\n",
    "\n",
    "$$\\frac{1}{2}||\\mathbf{w}||^2$$\n",
    "\n",
    "subject to the constraint that all data points are at least *1 margin width* away from the hyperplane (aka none of them are inside the margin).\n",
    "\n",
    "$$t_n(\\mathbf{w}^Tx_n + b) \\geq 1 \\hspace{0.25in} \\underbrace{\\forall n}_\\text{for all data points}$$\n",
    "\n",
    "\n",
    "We can shove this function + constraint into a Lagrangian:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\frac{1}{2} \\left \\| \\mathbf{w} \\right \\|^2 - \\sum_{n = 1}^N \\alpha_n \\left \\{ t_n(w^Tx_n + b)-1 \\right \\}$$\n",
    "\n",
    "and set the partial derivatives of $\\mathbf{w}$,$b$,and $\\mathbf{\\alpha}$ to 0 to find the optimal position of the hyperplane (defined by the weights $\\mathbf{w}$ and bias $b$).\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "## What the Lagrange multipliers can tell us\n",
    "When we use the Lagrangian to solve for our weights $\\mathbf{w}$, our bias $b$, and our Lagrange multipliers $\\alpha_n$, something *very* interesting pops out:\n",
    "\n",
    "$$a_n\\left \\{t_ny(x_n) - 1 \\right \\} = 0$$\n",
    "\n",
    "This formula says that the *product* of a data point's Lagrange multiplier $a_n$ and the point's distance to the hyperplane minus 1 (in other words $t_ny(x_n) - 1$) **must be zero**. The product of two numbers can only be zero when one or both of them is 0. So either:\n",
    "\n",
    "1. The distance between the point and the hyperplane is 1 (aka it's ON the margin) or\n",
    "2. its Lagrange multiplier is 0\n",
    "\n",
    "Remember, the Lagrange multiplier (sometimes called the \"shaddow price\" in econ) tells us the **impact a constraint has on the optima of a function**. For example, if you're maximizing your profit subject to a budget your business has, the Lagrange multiplier tells you how much more profit you could make, if you increased your budget a little. \n",
    "\n",
    "In the MMC case, this tells us that *only* support vectors influence the position of the hyperplane. All other points have a Lagrange multiplier of 0. For example, removing these points from the data set would not change where the hyperplane is located.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1m5GZObdjAFCjMwldvfyfSJkZPABYZr9c\" alt=\"hyperplane plot\" width = \"400\" class=\"center\"/> </center>\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\" class=\"center\"/>\n",
    "\n",
    "What would happen to the hyperplane if we removed one of the *support vectors*?\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "\n",
    "---\n",
    "$^1$ (we arbitrarily set the distance between the closest data point(s) and the hyperplane $\\underset{n}{\\min}\\left [ t_n(\\mathbf{w}^Tx_n + b) \\right ]$ to be 1 to make the math easier. Maximizing $\\frac{1}{||\\mathbf{w}||}$ is the same as minimizing $\\frac{1}{2}||\\mathbf{w}||^2$ )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Constraint/Budget vs. Penalty Formulation\n",
    "\n",
    "We can think of SVCs in two ways:\n",
    "\n",
    "1. As a penalty on the slack variables ($\\xi_i$)\n",
    "\n",
    "$$ \\text{arg min} \\frac{1}{2}\\left | \\mathbf{w} \\right |^2 + C \\sum_{i=0}^N \\xi_i$$\n",
    "\n",
    "Where $C$ controls how strongly we penalize non-zero slack variables.\n",
    "\n",
    "2. As a constraint on the sum of the slack variables ($\\xi_i$)\n",
    "$$ \\text{arg min} \\frac{1}{2}\\left | \\mathbf{w} \\right |^2 \\text{ subject to } \\sum_{i=0}^N \\xi_i \\leq C_{budget}$$\n",
    "\n",
    "(this is similar to how we can think of LASSO/Ridge as a penalty on the coefficients OR as a constraint/budget on how large our coefficients can be)\n",
    "\n",
    "# Making New Predictions\n",
    "In both Support Vector Classifiers and Maximal Margin classifiers, we classify a data point by multiplying its predictors by the weights ($\\mathbf{w}$) and adding the bias ($b$) and checking whether the value is > 0 (positive case, $t_n = 1$), or < 0 (negative case, $t_n = -1$).\n",
    "\n",
    "In math terms, we calculate $\\mathbf{w} \\cdot x_n + b$ and see if it is > 0 or < 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "# SVC in sklearn\n",
    "Let's build a Support Vector Classifier together using the `df_together` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blobs = make_blobs(n_samples = 100, random_state= 1234, centers = 2)\n",
    "df_together = pd.DataFrame(blobs[0])\n",
    "df_together.columns = [\"X1\", \"X2\"]\n",
    "df_together[\"y\"] = blobs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and organize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build empty model\n",
    "\n",
    "# fit model\n",
    "\n",
    "# assess performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/penguins.csv\")\n",
    "\n",
    "# get rid of chinstrap penguins\n",
    "df = df.loc[(df.species == \"Adelie\") |  (df.species == \"Gentoo\")]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Out\n",
    "Here's a simplified penguin dataset that only has Gentoo and Adelie Penguins (Chinstrap penguins have been removed).\n",
    "\n",
    "- Using ggplot, make a scatterplot of the bill length and bill depth data, coloring the points by species ([review of ggplot](https://github.com/cmparlettpelleriti/CPSC392ParlettPelleriti/blob/master/Lectures/LectureNotebooks/Visualization%20I--Class%204.ipynb))\n",
    "- Looking at the ggplot, imagine where YOU would draw a line separating these two groups\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "Looking at the data, can you see any reason why a Support Vector Classifier might be more desireable here than a Maximal Margin Classifier? Why? \n",
    "\n",
    "Where would the MMC put the dividing hyperplane? Where could SVC put it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# plot the ggplot\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split your data into a train/test split (80/20)\n",
    "- z-score your predictors\n",
    "- Build an empty `SVC()` model with `kernel = \"linear\"` and `C = 0.1`.\n",
    "- fit the model\n",
    "- plot the model using the function `plot_hyperplane()` that I wrote below\n",
    "- print out the train/test accuracies, and roc aucs. \n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "If you change the hyperparameter `C` to be 0.01, or 1, what happens to the margin? How many support vectors (points surrounded in red) are there with the different values of C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# organize and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# build fit assess model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### DON'T CHANGE JUST RUN TO LOAD FUNCTION ######################\n",
    "def plot_hyperplane(svm, X):\n",
    "    weights = svm.coef_[0]\n",
    "    bias = svm.intercept_[0]\n",
    "    slope = -weights[0]/weights[1]\n",
    "    intercept = -bias/weights[1]\n",
    "\n",
    "    margin = 1/np.sqrt(np.sum(weights**2))\n",
    "    lower_inter = intercept - (np.sqrt(1 + slope**2) * margin)\n",
    "    upper_inter = intercept + (np.sqrt(1 + slope**2) * margin)\n",
    "    \n",
    "    cols = X.columns\n",
    "    sv_df = pd.DataFrame(svm.support_vectors_)\n",
    "    sv_df.columns = cols\n",
    "    nice_cols = [c.replace(\"_\", \" \").title() for c in cols]\n",
    "    \n",
    "\n",
    "    a = (ggplot(X, aes(x = cols[0], y = cols[1])) +\n",
    "    geom_point() +\n",
    "    geom_abline(slope = slope, intercept = intercept,\n",
    "                color = \"red\", linetype = \"solid\", size = 1) + \n",
    "        geom_abline(slope = slope, intercept = lower_inter,\n",
    "                color = \"gray\", linetype = \"dotted\") +\n",
    "        geom_abline(slope = slope, intercept = upper_inter,\n",
    "                color = \"gray\", linetype = \"dotted\") + \n",
    "        theme_minimal() + \n",
    "        geom_point(data = sv_df, color = \"red\", size = 4, shape = \"o\", alpha = 0.25 ) + \n",
    "        labs(x = nice_cols[0],\n",
    "        y = nice_cols[1],\n",
    "        title = \"Hyperplane and Margins\") +\n",
    "        theme(legend_position= \"none\")) \n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plot_hyperplane function with your model and X_train\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "Try a few more values for C that are bigger than our biggest C and smaller than our smallest C, Which values of C lead you to have something that looks more like a Maximal Margin Classifier?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another SVM Example\n",
    "\n",
    "This time, use [this](https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/iris.csv) dataset to build a support vector machine using `sepal_width` and `sepal_length` to predict whether an iris flower is a `setosa` (coded as `1`), or `virginica` (coded as `-1`).\n",
    "\n",
    "We can pull the intercept (bias) and coefficients (weights) from the SVM using `model.coef_` and `model.intercept_` just like we did for Linear and Logistic Regression in CPSC 392!\n",
    "\n",
    "- Drop missing data (if any)\n",
    "- Split your data into a train/test split (80/20)\n",
    "- z-score your predictors\n",
    "- Build an empty `SVC()` model with `kernel = \"linear\"` and `C = 0.1`.\n",
    "- fit the model\n",
    "- print out the train/test accuracies, and roc aucs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "iris = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/iris.csv\")\n",
    "# get rid of chinstrap penguins\n",
    "\n",
    "# grab only Versi and Set irises\n",
    "iris = iris.loc[(iris.species == \"versicolor\") |  (iris.species == \"setosa\")]\n",
    "\n",
    "# head\n",
    "iris.head()\n",
    "\n",
    "# Do your train test split, and z score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# build fit assess\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin Width\n",
    "\n",
    "The formula for the margin width is:\n",
    "\n",
    "$$ \\frac{2}{\\lVert \\mathbf{w} \\rVert}$$\n",
    "\n",
    "Where $\\lVert \\mathbf{w} \\rVert$ refers to the L2 norm of the weights $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ ... \\\\ w_N\\end{bmatrix}$.\n",
    "\n",
    "Remember the L2 norm is calculated by:\n",
    "\n",
    "$$ L_2 \\text{ norm} = \\sqrt{\\sum_{i=1}^N w_i^2}$$\n",
    "\n",
    "\n",
    "Using this math, use `model.coef_` to grab the weights/coefficients and calculate the width of the margin for your model using python (Hint: take advantage of `numpy`'s many vectorized functions e.g. `np.sqrt()` or `np.linalg.norm()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the width of the margin\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Slack Variables\n",
    "Remember that Support Vector Classifiers improve upon Maximal Margin Classifiers by introducing slack variables $\\xi_i$ that allow data points to violate the margin or even be on the wrong side of the hyperplane. \n",
    "\n",
    "Using the model you built for the last question to calculate the slack variables for the data point $z$ which is a random sample from our training data. \n",
    "\n",
    "Slack variables are calculated as:\n",
    "\n",
    "$$\\xi_i = max(0, \\left |t_i - y(x_i) \\right |)$$\n",
    "\n",
    "Where $y(x_i)$ is the value $\\mathbf{w}*x_n + b$. \n",
    "\n",
    "- Use the `plot_hyperplane()` function I wrote to plot the hyperplane for the iris dataset. Then discuss with your group:\n",
    "\n",
    "1. Which regions of the graph have $\\mathbf{w}*x_n + b > 0$? Which regions have $\\mathbf{w}*x_n + b < 0$?\n",
    "2. Which regions have slack variables ($\\xi_i$) that are 0?\n",
    "3. Which regions have slack variables ($\\xi_i$) that are between 0 and 1?\n",
    "4. What would have to happen for a slack variable ($\\xi_i$) to be < 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "plot_hyperplane(svm_i, X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet Another SVC Example\n",
    "\n",
    "Using [this](https://raw.githubusercontent.com/cmparlettpelleriti/CPSC393ParlettPelleriti/main/Data/svmcw.csv) dataset, plot the data using ggplot to make a scatterplot of `X1` and `X2`, colored by `y` (the group of each data point).\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "Is an SVM *able* to do a good job on this dataset?\n",
    "\n",
    "\n",
    "Then, build an Support Vector Classifier to try to classify the data. How does it do?\n",
    "\n",
    "- Drop missing data (if any)\n",
    "- Split your data into a train/test split (80/20)\n",
    "- z-score your predictors\n",
    "- Build an empty `SVC()` model with `kernel = \"linear\"` and `C = 0.001`.\n",
    "- fit the model\n",
    "- print out the train/test accuracies, and roc aucs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "### YOUR PLOT HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your train test split, and z score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
